{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71011bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1cd2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a90738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b5da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "embedding_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "embedding_model.eval()\n",
    "\n",
    "for p in embedding_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c46e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeds = model_output.last_hidden_state\n",
    "    mask = attention_mask.unsqueeze(-1).expand(token_embeds.size()).float()\n",
    "    sum_emb = torch.sum(token_embeds * mask, dim=1)\n",
    "    sum_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    return sum_emb / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29092764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(text_list):\n",
    "    batch = tokenizer(text_list, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = embedding_model(**batch)\n",
    "        emb = mean_pooling(output, batch[\"attention_mask\"])\n",
    "    return emb.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c04bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_language(lang, df):\n",
    "    print(f\"\\n============= {lang.upper()} =============\")\n",
    "\n",
    "    texts = df[\"text\"].astype(str).tolist()\n",
    "    labels = df[\"label\"].astype(int).values\n",
    "\n",
    "    # Compute embeddings once\n",
    "    X = encode_texts(texts).numpy()\n",
    "    y = labels\n",
    "\n",
    "    k = 5\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    acc_list = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"Fold {fold+1}\")\n",
    "\n",
    "        # Split\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_train_t = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "        y_test_t = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "        # Classifier\n",
    "        clf = Classifier(X_train_t.shape[1]).to(device)\n",
    "        opt = torch.optim.Adam(clf.parameters(), lr=1e-3)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Train\n",
    "        clf.train()\n",
    "        loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=32, shuffle=True)\n",
    "        for epoch in range(5):\n",
    "            for bx, by in loader:\n",
    "                opt.zero_grad()\n",
    "                logits = clf(bx)\n",
    "                loss = loss_fn(logits, by)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "        # Evaluate\n",
    "        clf.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = clf(X_test_t)\n",
    "            pred = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        acc = np.mean(pred == y_test)\n",
    "        acc_list.append(acc)\n",
    "        print(f\"  Fold Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Compute mean/std + baseline\n",
    "    mean_acc = np.mean(acc_list)\n",
    "    std_acc = np.std(acc_list)\n",
    "    baseline = df[\"label\"].value_counts().max() / len(df)\n",
    "\n",
    "    return lang, mean_acc, std_acc, baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47292ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_files = {\n",
    "    \"amh\": \"amh.csv\",\n",
    "    \"arb\": \"arb.csv\",\n",
    "    \"deu\": \"deu.csv\",\n",
    "    \"eng\": \"eng.csv\",\n",
    "    \"fas\": \"fas.csv\",\n",
    "    \"hau\": \"hau.csv\",\n",
    "    \"hin\": \"hin.csv\",\n",
    "    \"ita\": \"ita.csv\",\n",
    "    \"nep\": \"nep.csv\",\n",
    "    \"spa\": \"spa.csv\",\n",
    "    \"tur\": \"tur.csv\",\n",
    "    \"urd\": \"urd.csv\",\n",
    "    \"zho\": \"zho.csv\",\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for lang, filename in language_files.items():\n",
    "    df = pd.read_csv(filename)\n",
    "    lang_result = evaluate_language(lang, df)\n",
    "    results.append(lang_result)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Print final summary table\n",
    "# -------------------------------\n",
    "summary = pd.DataFrame(results, columns=[\"Language\", \"Mean\", \"Std\", \"Baseline\"])\n",
    "print(\"\\n============== FINAL SUMMARY ==============\\n\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
